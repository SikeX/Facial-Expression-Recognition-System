{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.models import model_from_json\n",
    "from tkinter import filedialog\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型、表情导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global img_jpg          # 定义全局图像变量\n",
    "#var = tk.StringVar()    # 文字变量储存器\n",
    "\n",
    "model_path = './model/'\n",
    "img_size = 48\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "num_class = len(emotion_labels)\n",
    "\n",
    "# 从json中加载模型\n",
    "json_file = open(model_path + 'model_json.json')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# 加载模型权重\n",
    "model.load_weights(model_path + 'model_weight.h5')\n",
    "\n",
    "# 加载emotion\n",
    "emotion_images = {}\n",
    "for emoji in emotion_labels:\n",
    "    emotion_images[emoji] = cv2.imread(\"./emoji/\" + emoji + \".png\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face2emoji(face, emotion_index, position):\n",
    "    x, y, w, h = position\n",
    "    emotion_image = cv2.resize(emotion_images[emotion_index], (w, h))\n",
    "    overlay_img = emotion_image[:, :, :3]/255.0\n",
    "    overlay_bg = emotion_image[:, :, 3:]/255.0\n",
    "    background = (1.0 - overlay_bg)\n",
    "    face_part = (face[y:y + h, x:x + w]/255.0) * background\n",
    "    overlay_part = overlay_img * overlay_bg\n",
    "\n",
    "    face[y:y + h, x:x + w] = cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0)\n",
    "\n",
    "    return face\n",
    "\n",
    "def cam_fer():\n",
    "    # 创建VideoCapture对象\n",
    "    #video = \"http://admin:admin@192.168.31.18:8081/\"\n",
    "    capture = cv2.VideoCapture(0)\n",
    "\n",
    "    # 使用opencv的人脸分类器\n",
    "    cascade = cv2.CascadeClassifier(model_path + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "\n",
    "        # 灰度化处理\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # 呈现用emoji替代后的画面\n",
    "        emoji_show = frame.copy()\n",
    "\n",
    "        # 识别人脸位置\n",
    "        faceLands = cascade.detectMultiScale(gray, scaleFactor=1.1,\n",
    "                                             minNeighbors=1, minSize=(120, 120))\n",
    "\n",
    "        if len(faceLands) > 0:\n",
    "            for faceLand in faceLands:\n",
    "                x, y, w, h = faceLand\n",
    "                images = []\n",
    "                result = np.array([0.0] * num_class)\n",
    "\n",
    "                # 裁剪出脸部图像\n",
    "                image = cv2.resize(gray[y:y + h, x:x + w], (img_size, img_size))\n",
    "                image = image / 255.0\n",
    "                image = image.reshape(1, img_size, img_size, 1)\n",
    "\n",
    "                # 调用模型预测情绪\n",
    "                predict_lists = model.predict(image, batch_size=32, verbose=1)\n",
    "                # print(predict_lists)\n",
    "                result += np.array([predict for predict_list in predict_lists\n",
    "                                    for predict in predict_list])\n",
    "                # print(result)\n",
    "                emotion = emotion_labels[int(np.argmax(result))]\n",
    "                print(\"Emotion:\", emotion)\n",
    "                \n",
    "                emoji = face2emoji(emoji_show, emotion, (x, y, w, h))\n",
    "                cv2.imshow(\"Emotion\", emoji)\n",
    "                \n",
    "                # 框出脸部并且写上标签\n",
    "                cv2.rectangle(frame, (x - 20, y - 20), (x + w + 20, y + h + 20),\n",
    "                                (0, 255, 255), thickness=10)\n",
    "                cv2.putText(frame, '%s' % emotion, (x, y - 30),\n",
    "                            cv2.FONT_HERSHEY_DUPLEX, 2, (255, 255, 255), 2, 30)\n",
    "                cv2.imshow('Face', frame)\n",
    "            if cv2.waitKey(30) == 27:\n",
    "                break\n",
    "\n",
    "    # 释放摄像头并销毁所有窗口\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def vid_fer():\n",
    "    # 创建VideoCapture对象\n",
    "    video = \"./video/zhang.mp4\"\n",
    "    capture = cv2.VideoCapture(video)\n",
    "\n",
    "    # 使用opencv的人脸分类器\n",
    "    cascade = cv2.CascadeClassifier(model_path + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "    while True:\n",
    "        ret, frame = capture.read()\n",
    "\n",
    "        # 灰度化处理\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # 呈现用emoji替代后的画面\n",
    "        emoji_show = frame.copy()\n",
    "\n",
    "        # 识别人脸位置\n",
    "        faceLands = cascade.detectMultiScale(gray, scaleFactor=1.3,\n",
    "                                             minNeighbors=2, minSize=(225, 225))\n",
    "\n",
    "        if len(faceLands) > 0:\n",
    "            for faceLand in faceLands:\n",
    "                x, y, w, h = faceLand\n",
    "                images = []\n",
    "                result = np.array([0.0] * num_class)\n",
    "\n",
    "                # 裁剪出脸部图像\n",
    "                image = cv2.resize(gray[y:y + h, x:x + w], (img_size, img_size))\n",
    "                image = image / 255.0\n",
    "                image = image.reshape(1, img_size, img_size, 1)\n",
    "\n",
    "                # 调用模型预测情绪\n",
    "                predict_lists = model.predict(image, batch_size=32, verbose=1)\n",
    "                # print(predict_lists)\n",
    "                result += np.array([predict for predict_list in predict_lists\n",
    "                                    for predict in predict_list])\n",
    "                # print(result)\n",
    "                emotion = emotion_labels[int(np.argmax(result))]\n",
    "                print(\"Emotion:\", emotion)\n",
    "                \n",
    "                emoji = face2emoji(emoji_show, emotion, (x, y, w, h))\n",
    "                cv2.imshow(\"Emotion\", emoji)\n",
    "                \n",
    "                # 框出脸部并且写上标签\n",
    "                cv2.rectangle(frame, (x - 20, y - 20), (x + w + 20, y + h + 20),\n",
    "                              (0, 255, 255), thickness=10)\n",
    "                cv2.putText(frame, '%s' % emotion, (x, y + 20),\n",
    "                            cv2.FONT_HERSHEY_DUPLEX, 2, (255, 255, 255), 2, 30)\n",
    "                cv2.imshow('Face', frame)\n",
    "            if cv2.waitKey(30) == 27:\n",
    "                break\n",
    "\n",
    "    # 释放摄像头并销毁所有窗口\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file():\n",
    "    global path_\n",
    "    global img_ori\n",
    "    global img_plt1\n",
    "    global img_plt2\n",
    "    \n",
    "    path_ = filedialog.askopenfilename()\n",
    "    Img_ori = Image.open(path_).resize((254,254),Image.ANTIALIAS)\n",
    "    img_ori = ImageTk.PhotoImage(Img_ori)\n",
    "    label_Img = tk.Label(window, image=img_ori)\n",
    "    label_Img.pack(side=LEFT, padx=40)\n",
    "    \n",
    "    img = image.load_img(path_, grayscale=True, target_size=(48, 48))\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "\n",
    "    x /= 255\n",
    "\n",
    "    custom = model.predict(x)\n",
    "\n",
    "    x = np.array(x, 'float32')\n",
    "    x = x.reshape([48, 48]);\n",
    "\n",
    "    plt1 = plt.gray()\n",
    "    plt1 = plt.imshow(x)\n",
    "    #plt.show()\n",
    "    plt1.figure.savefig(\"./plt/2.png\")\n",
    "    img_plt2 = Image.open(\"./plt/2.png\")\n",
    "    img_plt2 = ImageTk.PhotoImage(img_plt2)\n",
    "    label_plt2 = tk.Label(window, image=img_plt2)\n",
    "    label_plt2.pack(side=LEFT, padx=20)\n",
    "    \n",
    "    emotion_analysis(custom[0])\n",
    "\n",
    "def emotion_analysis(emotions):\n",
    "    global img_plt1\n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    ax.set_xticks(y_pos)\n",
    "    ax.set_xticklabels(objects)\n",
    "    ax.set_ylabel('percentage')\n",
    "    ax.set_title('emotion')\n",
    " \n",
    "    #plt.show()\n",
    "    ax.figure.savefig(\"./plt/1.png\")\n",
    "    img_plt1 = Image.open(\"./plt/1.png\")\n",
    "    img_plt1 = ImageTk.PhotoImage(img_plt1)\n",
    "    label_plt1 = tk.Label(window, image=img_plt1)\n",
    "    label_plt1.pack(side=LEFT, padx=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主程序及GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-805bd3792f40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    window=tk.Tk(\"haha\")   \n",
    "    #img = ImageTk.PhotoImage(Image.open(\"logo.png\"))\n",
    "    \n",
    "    window.geometry(\"1400x900+100+10\") \n",
    "    window.configure(background ='white') \n",
    "    window.grid_rowconfigure(0, weight = 1) \n",
    "    window.grid_columnconfigure(0, weight = 1) \n",
    "    \n",
    "    #Head\n",
    "    message = tk.Label( \n",
    "    window, text =\"Expression-Recognition-System\",  \n",
    "    bg =\"green\", fg = \"white\", width = 30,  \n",
    "    height = 3, font = ('arial', 30, 'bold'))  \n",
    "      \n",
    "    message.place(x = 200, y = 20) \n",
    "    \n",
    "    takeImg = tk.Button(window, text =\"Picture\",  \n",
    "    command = get_file, fg =\"white\", bg =\"green\",  \n",
    "    width = 10, height = 3, activebackground = \"Red\",  \n",
    "    font =('times', 15, ' bold ')) \n",
    "    takeImg.place(x = 200, y = 700)\n",
    "    \n",
    "    \n",
    "    trainImg = tk.Button(window, text =\"Camra\",  \n",
    "    command = cam_fer, fg =\"white\", bg =\"green\",  \n",
    "    width = 10, height = 3, activebackground = \"Red\",  \n",
    "    font =('times', 15, ' bold ')) \n",
    "    trainImg.place(x = 500, y = 700)\n",
    "    \n",
    "    trackImg = tk.Button(window, text =\"Video\",  \n",
    "    command = vid_fer, fg =\"white\", bg =\"green\",  \n",
    "    width = 10, height = 3, activebackground = \"Red\",  \n",
    "    font =('times', 15, ' bold ')) \n",
    "    trackImg.place(x = 800, y = 700) \n",
    "    quitWindow = tk.Button(window, text =\"Quit\",  \n",
    "    command = window.destroy, fg =\"white\", bg =\"Red\",  \n",
    "    width = 10, height = 3, activebackground = \"Blue\",  \n",
    "    font =('times', 15, ' bold ')) \n",
    "    quitWindow.place(x = 1100, y = 700) \n",
    "    \n",
    "        \n",
    "    window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-7-7fc382fc0b82>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-7fc382fc0b82>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Button(frame,text=\"选择扰动\",command=get_trg,height=1,width=8,bg='#1ebad6',font=('微软雅黑',14)).grid(row=12,column=1,padx=10,sticky=S)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#选择扰动按钮\n",
    "    Button(frame,text=\"选择扰动\",command=get_trg,height=1,width=8,bg='#1ebad6',font=('微软雅黑',14)).grid(row=12,column=1,padx=10,sticky=S)\n",
    "\n",
    "#添加显示原始图片的背景图片\n",
    "    #img1 = Image.open(\"picbg.png\")\n",
    "    photo1 = ImageTk.PhotoImage(img1)\n",
    "    img_label1 = Label(frame, imag=photo1,bd=0)\n",
    "    img_label1.grid(row=4,column=2,rowspan=3,columnspan=3,padx=4,pady=10)\n",
    "\n",
    "#添加显示合成图片的背景图片\n",
    "    img2 = Image.open(\"picbg.png\")\n",
    "    photo2 = ImageTk.PhotoImage(img2)\n",
    "    img_label2 = Label(frame, imag=photo2,bd=0)\n",
    "    img_label2.grid(row=12,column=2,rowspan=3,columnspan=3,padx=4,pady=20)\n",
    "\n",
    "#显示图片标签\n",
    "    frm_L0 = Frame(frame)\n",
    "    var0 = StringVar()\n",
    "    Label(frm_L0,text=\"图片标签：\").pack(side=LEFT)\n",
    "    Entry(frm_L0,textvariable=var0,bd=0).pack(side=LEFT)\n",
    "    frm_L0.grid(row=7,column=3)\n",
    "\n",
    "#选择原始图片训练模型\n",
    "    Button(frame, text=\"原始模型\", command=can_data1, height=1, width=8,font=('微软雅黑',12),activebackground='pink', bg='#161626',fg='#f2f2f4',bd=1).grid(row=4, column=6,padx=4,pady=10)\n",
    "    Button(frame, text=\"修改模型\", command=can_data2, height=1, width=8,font=('微软雅黑',12),activebackground='pink', bg='#c0c0c8',bd=1).grid(row=5, column=6,padx=4,pady=10)\n",
    "#选择合成图片训练模型\n",
    "    Button(frame, text=\"原始模型\", command=can_data3, height=1, width=8,font=('微软雅黑',12),activebackground='pink', bg='#161626',fg='#f2f2f4',bd=1).grid(row=12, column=6,padx=4,pady=10)\n",
    "    Button(frame, text=\"修改模型\", command=can_data4, height=1, width=8,font=('微软雅黑',12),activebackground='pink', bg='#c0c0c8',bd=1).grid(row=13, column=6,padx=4,pady=10)\n",
    "\n",
    "#设置原始图片处理\n",
    "#设置原始模型预测结果的显示框\n",
    "    nfrm_L1 = Frame(frame)\n",
    "    var11 = StringVar()\n",
    "    var11.set(\"\")\n",
    "    Label(nfrm_L1, text=\"分类结果：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(nfrm_L1, textvariable=var11,bd=0).pack()\n",
    "    nfrm_L1.grid(row=4, column=8,padx=2)\n",
    "\n",
    "#设置原始预测结果置信度的显示框\n",
    "    nfrm_L2 = Frame(frame)\n",
    "    var12 = StringVar()\n",
    "    var12.set(\"\")\n",
    "    Label(nfrm_L2, text=\"置信度：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(nfrm_L2, textvariable=var12, bd=0).pack()\n",
    "    nfrm_L2.grid(row=4,column=9,padx=2)\n",
    "\n",
    "#设置修改模型预测结果的显示框\n",
    "    mfrm_L1 = Frame(frame)\n",
    "    var13 = StringVar()\n",
    "    var13.set(\"\")\n",
    "    Label(mfrm_L1,text=\"分类结果：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(mfrm_L1,textvariable=var13,bd=0).pack()\n",
    "    mfrm_L1.grid(row=5,column=8,padx=2)\n",
    "\n",
    "#设置修改预测结果置信度的显示框\n",
    "    mfrm_L2 = Frame(frame)\n",
    "    var14 = StringVar()\n",
    "    var14.set(\"\")\n",
    "    Label(mfrm_L2, text=\"置信度：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(mfrm_L2, textvariable=var14, bd=0).pack()\n",
    "    mfrm_L2.grid(row=5,column=9,padx=2)\n",
    "\n",
    "#设置合成图片处理\n",
    "#设置原始模型预测结果的显示框\n",
    "    nfrm_L3 = Frame(frame)\n",
    "    var21 = StringVar()\n",
    "    var21.set(\"\")\n",
    "    Label(nfrm_L3, text=\"分类结果：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(nfrm_L3, textvariable=var21, bd=0).pack()\n",
    "    nfrm_L3.grid(row=12, column=8)\n",
    "\n",
    "#设置原始预测结果置信度的显示框\n",
    "    nfrm_L4 = Frame(frame)\n",
    "    var22 = StringVar()\n",
    "    var22.set(\"\")\n",
    "    Label(nfrm_L4, text=\"置信度：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(nfrm_L4, textvariable=var22, bd=0).pack()\n",
    "    nfrm_L4.grid(row=12, column=9)\n",
    "\n",
    "#设置修改模型预测结果的显示框\n",
    "    mfrm_L3 = Frame(frame)\n",
    "    var23 = StringVar()\n",
    "    var23.set(\"\")\n",
    "    Label(mfrm_L3,text=\"分类结果：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(mfrm_L3,textvariable=var23,bd=0).pack()\n",
    "    mfrm_L3.grid(row=13, column=8)\n",
    "\n",
    "#设置修改模型预测结果置信度的显示框\n",
    "    mfrm_L4 = Frame(frame)\n",
    "    var24 = StringVar()\n",
    "    var24.set(\"\")\n",
    "    Label(mfrm_L4, text=\"置信度：\",bg='#c0c0c8').pack(fill=BOTH)\n",
    "    Entry(mfrm_L4,textvariable=var24,bd=0).pack()\n",
    "    mfrm_L4.grid(row=13, column=9)\n",
    "\n",
    "\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
